import React from "react";
import { Grommet, Box, Button, Heading } from "grommet";
import { grommet } from "grommet/themes";
import "./App.css";
import Webcam from "react-webcam";
import request from "request";
import DisplayFaceData from "./components/DisplayFaceData/DisplayFaceData";

class App extends React.Component {
  state = {
    imageData: null,
    image_name: "",
    saveImage: false,
    sendImage: ""
    //detectFaceData: [],
    //faceData: [],
    //faceRects: [],
    //personList: []
  };
  setRef = webcam => {
    this.webcam = webcam;
  };

  /*   capture = () => {
    const imageSrc = this.webcam.getScreenshot();
    this.setState({
      imageData: imageSrc
    });
  }; */

  handleSubmit = async (event) => {
    event.preventDefault();
    var personGroupName = "hpe";
    let detectFaceData = [];
    let faceRects = [];
    let personList = [];
    let faceData = [];

    //capture the image 
    const imageSrc = this.webcam.getScreenshot();
    this.setState({
      imageData: imageSrc
    });

    // Azure face api subscription key
    const subscriptionKey = "27e14365dc7f49eeb8a6000bd44c53fe";
    /* const imageUrl =
      "https://upload.wikimedia.org/wikipedia/commons/3/37/Dagestani_man_and_woman.jpg"; */
    
    // URI for face detect
    const uri_Detect =
      "https://azure-faceapi.cognitiveservices.azure.com/face/v1.0/detect";
    // URI for face Identify  
    const uri_Identify =
      "https://azure-faceapi.cognitiveservices.azure.com/face/v1.0/identify?";

    const dataURItoBuffer = function(dataURL, callback) {
      console.log("Entered dataURItoBuffer");
      const buff = new Buffer(
        dataURL.replace(/^data:image\/(png|gif|jpeg);base64,/, ""),
        "base64"
      );
      callback(buff);
    }; 

    const dataURItoBuffer = function(dataURL, callback) {
      console.log("Entered dataURItoBuffer");
      const buff = new Buffer(
        dataURL.replace(/^data:image\/(png|gif|jpeg);base64,/, ""),
        "base64"
      );
      callback(buff);
    };

    dataURItoBuffer(imageSrc, function(buff) {
      // Request parameters.
      let params_Detect = {
        returnFaceId: "true",
        returnFaceLandmarks: "false",
        returnFaceAttributes:
          "age,gender,smile,facialHair,glasses,emotion," + "hair"
      };
      request.post(
        {
          url:
            uri_Detect +
            "?returnFaceId=true&returnFaceLandmarks=false&returnFaceAttributes=age,gender,smile,facialHair,glasses,emotion,hair",
          headers: {
            "Content-Type": "application/octet-stream",
            "Ocp-Apim-Subscription-Key": subscriptionKey
          },
          body: buff
        },
        function(err, httpResponse, body) {
          console.log("detect body", JSON.parse(body));
          console.log("identify length", JSON.parse(body).length);
          detectFaceData = JSON.parse(body);
          checkData(JSON.parse(body));
          getFaceRects(JSON.parse(body));
          displayData(JSON.parse(body));
        }
      );
      /*       fetch(uri_Detect, {
        method: "POST",
        body: buff,
        headers: {
          "Content-Type": "application/octet-stream",
          "Ocp-Apim-Subscription-Key": subscriptionKey
        },
        credentials: "same-origin"
      }).then(
        function(response) {
          console.log("fetch response", response);
          response.text().then(function(output) {
            console.log("fetch response text", output);
            //this.setState({ detectFaceData: JSON.parse(output) });
            detectFaceData = JSON.parse(output);
            checkData(JSON.parse(output));
            getFaceRects(JSON.parse(output));
          });
          //return response.text()
        },
        function(error) {
          //error.message //=> String
          console.log("dataURItoBuffer func error", error);
        }
      ); */
    });

    function checkData(body) {
      var faceIds = [];
      console.log("body kength", body.length);
      if (body.length !== 0) {
        //var faceIds = data.map((person) => person.faceId);
        for (var i = 0; i < body.length; i++) {
          faceIds.push(body[i].faceId);
        }
        console.log("checkData faceid list", faceIds);
        identifyFace(faceIds);
      }
    }
    // Call this after detecting faces, with face IDs,
    // to identify the faces from the people group
    function identifyFace(faceIds) {
      request.post(
        {
          url: uri_Identify,
          headers: {
            "Content-Type": "application/json",
            "Ocp-Apim-Subscription-Key": subscriptionKey
          },
          body: JSON.stringify({ faceIds: faceIds, personGroupId: "hpe" })
        },
        function(err, httpResponse, body) {
          console.log("identify body", JSON.parse(body));
          console.log("identify length", JSON.parse(body).length);
          displayData(JSON.parse(body));
        }
      );
    }

    // Save and display the face data
    function displayData(personIdData) {
      //var data = this.state.detectFaceData;
      var data = detectFaceData;
      console.log("inside displayData data", data);
      var name;
      var confidence;
      //var faceDataArray = [];
      for (var i = 0; i < data.length; i++) {
        if (personIdData[i].candidates[0] != null) {
          name = getNameFromId(personIdData[i].candidates[0].personId);
          confidence = parseInt(personIdData[i].candidates[0].confidence * 100);
        } else {
          name = "unknown";
          confidence = "N/A";
        }
        var att = data[i].faceAttributes;
        var gender = att.gender.toString();
        var age = att.age;
        var smile = (parseFloat(att.smile) * 100).toFixed(1);
        var emotions = "";
        // Return the most confident emotion
        emotions = Object.keys(att.emotion).reduce(function(a, b) {
          return att.emotion[a] > att.emotion[b] ? a : b;
        });
        if (emotions === "") {
          emotions = "unclear";
        }
        var hair = "";
        // Return the most confident hair color
        if (att.hair.invisible) hair = "unclear";
        else if (att.hair.bald > 0.7) hair = "bald";
        // Microsoft returns the hair color in an array in order of confidence
        else {
          hair = att.hair.hairColor[0].color;
        }
        faceData.push({
          name: name,
          confidence: confidence,
          gender: gender,
          age: age,
          smile: smile,
          emotions: emotions,
          hair: hair
        });
      }
      //this.setState({ faceData: faceDataArray });
      console.log("inside display data", faceData);
    }

    // API call to add the current face image to a specific person

    function addPersonFace(imageDataURL, personId) {
      dataURItoBuffer(imageSrc, function(buff) {
        const uri_PersonGroups =
          "https://azure-faceapi.cognitiveservices.azure.com/face/v1.0/persongroups/" +
          personGroupName +
          "/persons/" +
          personId +
          "/persistedFaces?";
        const params_PersonGroups =
          "returnFaceId=true&returnFaceLandmarks=false&returnFaceAttributes=age,gender,smile,facialHair,glasses,emotion,hair";
        request.post(
          {
            url: uri_PersonGroups + params_PersonGroups,
            headers: {
              "Content-Type": "application/octet-stream",
              "Ocp-Apim-Subscription-Key": subscriptionKey
            },
            body: buff
          },
          function(err, httpResponse, body) {
            console.log("addPersonFace body", JSON.parse(body));
            console.log("addPersonFace length", JSON.parse(body).length);
            alert("added face for " + getNameFromId(personId));
            displayData(JSON.parse(body));
          }
        );
      });
    }

    function getNameFromId(personId) {
      for (let i = 0; i < personList.length; i++) {
        if (personList[i][1] === personId) return personList[i][0];
      }
    }

    // CLear, create and save the person list
    function savePersonList(data) {
      //let personListArray = [];
      for (var i = 0; i < data.length; i++) {
        personList.push([data[i].name, data[i].personId]);
      }
      //this.setState({ personList: personListArray });
    }

    // Get the face location rectangle coordinate/size information
    function getFaceRects(data) {
      //var faceRectsArray = [];
      for (var i = 0; i < data.length; i++) {
        var faceRect = data[i].faceRectangle;
        faceRects.push({
          left: faceRect.left,
          top: faceRect.top,
          width: faceRect.width,
          height: faceRect.height
        });
      }
      //this.setState({ faceRects: faceRectsArray });
      //faceRects = faceRectsArray;
      console.log("getFaceRects", faceRects);
      //drawImage();
    }

    /* // Draw the image with the face location rectangles
function drawImage() {
  // First draw the face location rectangles on to the canvas
  console.log("face reacts in draw image", this.state.faceRects);
  ctx.strokeStyle = "#ffbb00";
  ctx.lineWidth = 2;
  for (var i = 0; i < this.state.faceRects.length; i++) {
    // highlight the face with info displayed on the right
    if (i == faceIndex)
      ctx.lineWidth = 6;
    ctx.strokeRect(this.state.faceRects[i].left, this.state.faceRects[i].top, this.state.faceRects[i].width, this.state.faceRects[i].height);
    ctx.lineWidth = 2;
  }
  // Then draw the whole image
  var imageDataURL = cvs.toDataURL("image/jpeg");
  $("#screenGrab").attr("src", imageDataURL);
}
 */
  };

  render() {
    const videoConstraints = {
      width: 1280,
      height: 720,
      facingMode: "user"
    };
    var cvs = document.createElement("canvas");
    //var cvsMotion = document.createElement("canvas");
    var cvsDiff = document.createElement("canvas");
    var ctx = cvs.getContext("2d");
    //var ctxMotion = cvsMotion.getContext("2d");
    var ctxDiff = cvsDiff.getContext("2d");
    cvsDiff.width = 640;
    cvsDiff.height = 480;

    return (
      <Grommet theme={grommet}>
        <Box
          direction="row-responsive"
          justify="center"
          align="center"
          pad="large"
          background="dark-2"
          gap="medium"
        >
          <form onSubmit={this.handleSubmit}>
            <Heading margin="none" textAlign="center">
              Face Detection App
            </Heading>
            <Box
              pad="medium"
              align="center"
              //background={{ color: "light-2", opacity: "strong" }}
              gap="small"
            >
              <Webcam
                audio={false}
                height={350}
                ref={this.setRef}
                screenshotFormat="image/jpeg"
                width={350}
                videoConstraints={videoConstraints}
              />
              <Button type="submit" primary label="Capture Photo" />
            </Box>
            {this.state.imageData ? (
              <Box>
                <img src={this.state.imageData} />
                <DisplayFaceData />
              </Box>
            ) : null}
          </form>
        </Box>
      </Grommet>
    );
  }
}

export default App;
